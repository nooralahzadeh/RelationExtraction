{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import nltk,re\n",
    "from nltk import word_tokenize \n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import operator\n",
    "import io\n",
    "import os\n",
    "import array\n",
    "from six.moves import cPickle as pickle\n",
    "from datetime import datetime\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path='/user/fnoorala/home/Desktop/SMILK/InformationExtraction/data/annotated_cosmetic_corpus.xml'\n",
    "pathtest='/user/fnoorala/home/Desktop/SMILK/InformationExtraction/data/test.xml'\n",
    "SENTENCE_START_TOKEN = \"SENTENCE_START\"\n",
    "SENTENCE_END_TOKEN = \"SENTENCE_END\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\"\n",
    "word_to_index = []\n",
    "index_to_word = []\n",
    "vocabulary_size=8000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readfiles(filename):\n",
    "     with codecs.open(filename, encoding='utf-8') as f:\n",
    "        lines = [line.rstrip('\\n') for line in open(filename) \n",
    "                 if (len(line.strip())>0)]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractTags(mystr):\n",
    "        tags=[ match.group(1) for match in re.finditer('<(?P<tag>\\w+)>(.*?)</(?P=tag)>',mystr)]\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replacement(x):\n",
    "    ne= x.group(1)\n",
    "    tokens=nltk.word_tokenize(x.group(2))\n",
    "    rep=\"\"\n",
    "    for j in range(len(tokens)):\n",
    "        rep+=(\"B+\"+ne if j==0 else \"I+\"+ne)+\" \"\n",
    "    return rep.strip()\n",
    "def clearTags(x):\n",
    "    tokens=nltk.word_tokenize(x.group(2))\n",
    "    rep=\"\"\n",
    "    for j in range(len(tokens)):\n",
    "        rep+=(tokens[j] if j==0 else tokens[j])+\" \"\n",
    "    return rep.strip()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformation(filename):\n",
    "    dataset=readfiles(filename)\n",
    "    texts=[]\n",
    "    reltags=[]\n",
    "    entitytags=[]\n",
    "    \n",
    "    for k in range(0,len(dataset)):\n",
    "        \n",
    "        result_d = {}\n",
    "        text=dataset[k].decode(\"utf-8\")\n",
    "        rep1=re.sub('<(?P<tag>\\w+)>(.*?)</(?P=tag)>',clearTags,text)\n",
    "        rep2=re.sub('<(?P<tag>\\w+)>(.*?)</(?P=tag)>',replacement,text)\n",
    "        rg = re.compile('[BI]\\+(?:[A-Za-z][a-z0-9_]*)')\n",
    "        t=nltk.word_tokenize(rep1)\n",
    "        tokens=nltk.word_tokenize(rep2)\n",
    "        \n",
    "        words= [x.lower() for x in t]\n",
    "        lables=[ x if rg.match(x) is not None else 'O' for x in tokens ]\n",
    "        texts.append(words)\n",
    "        reltags.append(lables)\n",
    "     \n",
    "    return texts,reltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset(path):\n",
    "    \n",
    "    train=''\n",
    "    \n",
    "    train_set, train_labels=transformation(train)\n",
    "      \n",
    "    \n",
    "    #sentences = ['%s %s %s' % (SENTENCE_START_TOKEN, x, SENTENCE_END_TOKEN) for x in sentences]\n",
    "    #labels=[[SENTENCE_START_TOKEN]+l+[SENTENCE_END_TOKEN] for l in labels]\n",
    "\n",
    "    # Tokenize the sentences into words\n",
    "    \n",
    "    train_tokenized_sentences = [nltk.word_tokenize(sent) for sent in train_set]\n",
    "    \n",
    "    #we converted sequences of numbers with the string DIGIT i.e. 1984\n",
    "    #is converted to DIGITDIGITDIGITDIGIT.\n",
    "  \n",
    "    train_tokenized_sentences=[[re.sub(r\"\\d+\",lambda m:convertToDigit(m.group()),w) for w in t ] for t in train_tokenized_sentences]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Count the word frequencies\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*train_tokenized_sentences))\n",
    "    \n",
    "    print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "    # Get the most common words and build index_to_word and word_to_index vectors\n",
    "    vocab = sorted(word_freq.items(), key=lambda x: (x[1], x[0]), reverse=True)[:vocabulary_size-2]\n",
    "\n",
    "    #we deal with unseen words in the test set by marking any words with only one \n",
    "    #single occurrence in the training set as <UNK>\n",
    "    vocab = [x for x in vocab if not x[1]<2]\n",
    "\n",
    "    #print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "    sorted_vocab = sorted(vocab, key=operator.itemgetter(1))\n",
    "   \n",
    "    index_to_word = [\"<MASK/>\", UNKNOWN_TOKEN] + [x[0] for x in sorted_vocab]\n",
    "    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "\n",
    " \n",
    "    # Replace all words not in our vocabulary with the unknown token\n",
    "    for i, sent in enumerate(train_tokenized_sentences):\n",
    "        train_tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]\n",
    "    for i, sent in enumerate(test_tokenized_sentences):\n",
    "        test_tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]\n",
    "\n",
    "    lables_1=[item for sublist in train_labels_1 for item in sublist]\n",
    "    lables_2=[item for sublist in train_labels_2 for item in sublist]\n",
    "    \n",
    "    \n",
    "    \n",
    "    index_to_label_1=['<MASK/>']+[i for i in np.unique(lables_1)]\n",
    "    index_to_label_2=['<MASK/>']+[i for i in np.unique(lables_2)]\n",
    "    \n",
    "    label_to_index_1 = dict([(w, i) for i, w in enumerate(index_to_label_1)])\n",
    "    label_to_index_2 = dict([(w, i) for i, w in enumerate(index_to_label_2)])\n",
    "    \n",
    "    dicts={'words2idx':word_to_index,'labels2idx_1':label_to_index_1,'labels2idx_2':label_to_index_2}\n",
    "    # Create the training dataset and test dataset\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent] for sent in train_tokenized_sentences])\n",
    "    y1_train = np.asarray([[label_to_index_1[w] for w in sent] for sent in train_labels_1])\n",
    "    y2_train = np.asarray([[label_to_index_2[w] for w in sent] for sent in train_labels_2])\n",
    "      \n",
    "    X_test = np.asarray([[word_to_index[w] for w in sent] for sent in test_tokenized_sentences])\n",
    "    y1_test = np.asarray([[label_to_index_1[w] for w in sent] for sent in test_labels_1])\n",
    "    y2_test = np.asarray([[label_to_index_2[w] for w in sent] for sent in test_labels_2])\n",
    "                        \n",
    "    \n",
    "    semeval={'train_dataset':X_train,'train_labels_1':y1_train,'train_labels_2':y2_train,'test_dataset':X_test,\n",
    "            'test_labels_1':y1_test,'test_labels_2':y2_test,'dicts':dicts}\n",
    "    return semeval\n",
    "#save the train, test and dictionary in a serialized object\n",
    "pickle.dump( dataset(path), open( \"semeval.pkl\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
